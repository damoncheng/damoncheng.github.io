## Xen Live Migration ##

### Design ###
- 在线迁移所焦点的物理资源:
	
	memory, network, disk

- Migrating Memory

	- 要求: 
		
		方法要能够平衡downtime和total migration time. 最小化两个时间.
	     
	- 在线迁移内存迁移的三个阶段:
		
		Push phase 

		Stop-and-copy phase
	
		Pull phase : The new VM excutes and, if it accesses a page that has not yet been copied, this page is fauted in ("pulled") across the network from source VM.

	- 大多数实际解决方案选择三个中的一个或者两个.

		如果仅仅Stop-and-copy阶段,算法退化为离线拷贝

		本论文绑定第一阶段和第二阶段

	- 本论文引入了WWS(writable working set)的概念,来合理确定第一阶段的循环次数.

	- Pre-copy的一个问题是由于不断的重传内存,可能消耗宽带,从而影响集群提供服务.

- Local Resources

	-  清晰概念:

		在内存被迁移后, network 和 local storage如何被关联

	- 为了解决网络接口问题, migrated host生成一个未经请求的ARP回应, 来公告IP已经被移到新的地方, 从而更新交换机和路由器路由表, 这个可能导致少量传播中的包丢失, 但不会引起显著的中断.

	- 一些路由器被配置不接受广播ARP回应(为了防止IP洪),因此一个未经请求的ARP回应并不能应用到任何场景.

	- storage的迁移可以被类似处理,而大多数的modern data centers通过NAS来满足存储需求.个别服务偏爱local disks. NAS对于migrate有独特的优势.


- Design Overview
  
	- 迁移的5个阶段:
	
		Stage 0: Pre-Migration

		Stage 1: Reservation

		Stage 2: Iterative Pre-Copy

		Stage 3: Stop-and-Copy

		Stage 4: Commitment

		Stage 5: Activation

	- 逻辑图:

		![logical_step](./Picture/logical_step.png)

### Writable Working Set ###

- 预拷贝问题:

	- 怎么判断什么时候停止预拷贝, 由于太多的时间和资源正在被浪费?
	  尤其是在脏页产生率快于copy传输率的时候, 应该立即停止预拷贝.

- writable working set(WWS)的概念 : 

	- while the remainder will be written
often and so should best be transferred via stop-and-copy - we dub this latter set of pages the writable working set(WWS) of the operating system by obvious extension of the original working set concept[17]

- 通过分析操作系统(运行在不同的负荷)的WWS来尝试进行一些洞悉,从而来建立一个启发式的,且有效的和可控的预拷贝算法.


- Measuring Writable Working Sets

	- 通过一个特别执行的操作系统, 使用Xen的 shadow page tables 来追踪进行脏页统计, 从而允许我们判断任何时间周期里被写的页集.

	- 测试环境

		a dual processor Intel Xeon 2.4GHz, 512MB内存, 执行得到的每个基准都来自于独立的一个VM, 每50ms从另一个虚拟机中读dirty bitmap, 每8s清除一次, 从而允许我们每8s窗口计算并展示一次WWS,但是可以50ms的粒度来评估做相关事情.

	- 允许程序

		SPEC CINT2000 : 通过按序执行一系列小程序来测试整个时间范围的结果. 其中x-axis来测量elapsed time, y-axis显示每8秒4kb大小的脏页数量, 并带有sub-benchmark程序标注, 从图中可以看到, eon的WWS有很少量的工作集合, 因此适合迁移, 而gap持续有高脏页率, 因此不适合迁移. **因此执行一个操作系统的迁移依赖于workload和开始迁移的the precise moment, 从而会产生完全不同的结果**

	![measure_wws_spec](./Picture/measure_wws_spec.png)		

		OSDB OLTP benchmark using Post-greSQL and SPECweb99 using apache

		a Quake 3 server as we are particularly interested in  highly interactive workloads.

	
- Estimating Migration Effectiveness

	- 使用追踪到的数据来评估预拷贝算法在不同workload的效率.

	- 展示其中一副基准图(一类workload):
		![Estimate effective](./Picture/estimate_migration_effectiveness.png)
		
	- 相关含义:
	
		* 四条这线代表四次实验
		* 灰色柱线与折线的交点为一次迭代
		* 左边纵坐标表示此时停机拷贝的downtime
		* 右边坐标表示该轮脏页产生速率(pages/sec)
		* 三个graph是在128Mbit/s,256Mbit/s,512Mbit/s下网速的分别实			 验

	- 实验表现:
		预拷贝算法与完全的stop-copy算法相比,其downtime有明显的缩短.
		512M内存下, 分别在128Mbit/s,256Mbit/s,512Mbit/s网络速度下			的完全stop-copy的downtime分别为:32s, 16s, 8s


- Implement Issues
	
	- 我们设计和实现我们的预拷贝迁移方法结合Xen virtual machine monitor.

	- xen安全的划分主机资源成为一个个独立的虚拟机资源, 每一个允许一个OS instance, 此外有一个特别的虚拟机负责管理和控制机器.

	- 我们用两个不同的方法初始化和管理状态转换,这是设计空间的两个极端点 : 
		
		1. 在management VM中的一个迁移守护进程负责迁移,即管理迁移实现在外部.(managed migration)
		2. 相反的,  另一中是完全在迁移OS里面实现迁移, with only a small stub required on the destination machine.(self migration)

	- Managed Migration

		* Managed migration靠源和目的主机management VMs中的migration daemons来实现迁移. 他们负责在目的主机创建新的VM, 协调进行网络的在线传输.

		* 预拷贝轮寻阶段, 每轮轮寻的开始,每次从Xen中拷贝脏页位图.
  
		* 由于每个guest OS对page tables的操作都会由处理器的MMU(内存管理单元)填充TLB(页表缓冲), 从而可以使得guest OS被暴露到物理地址变得可能, 因此创建的page tables不需要被Xen映射成物理地址.

		* 脏页位图的形成:

			对于记录脏页的过程, Xen在运行的OS下插入shadow page tables, shadow page tables是guest page tables的翻译部分, 具体流程是: all page-table entries(PTEs) 被初始化为read-only来被映射到shadow tables当中, 无论guest tables中的PTE是什么权限, 当guest OS尝试修改一个内存的页面时,Xen将捕捉到page fault. 如果这个write permission被相关的guest PTE满足, 这个PTE的写权限将被扩展到shadow PTE. 同一时间, 我们设定VM脏页位图的相关bit.

			在每轮中,随着位图被拷贝到control software, Xen的bitmap被重置, shadow page tables被摧毁然后被重建. 使得原来的写权限丢失, 新一轮的记录开始.

	- Self Migration
		
		* 与managed migration相反, self migration将迁移的主要实现放置到OS里面. 这个设计不需要对source machine的Xen或者management software进行修改, 尽管在destination machine需要一个miagration stub来监听迁移请求, 创建一个相关的空VM, 接受被迁移系统的状态.

		* self migraion基础上实现的Pre-coping schecme在概念上与managed migration类似, 主要的不同来自stop-and-copy阶段, 由于迁移软件运行在OS里面, OS不能停止, 因此将stop-and-copy拆分为两个阶段, 第一个阶段将关闭除了migration外所有的OS activity, 然后获取脏页位图进行最后一次扫描, 将第一阶段产生的脏也复制到shadow buffer中. 第二阶段则将shadow buffer中的内容传输, 忽视期间产生的脏页.

   
	- Dynamic Rate-Limiting

		* 选择一个单一的网络带宽限制不是合适的,尽管一个低的限制避免对运行中服务的性能影响. 但是会延迟downtime和总运行时间.

		* 随着每轮迭代动态调整带宽限制, 管理员选择一个minimum 和 一个maximum带宽. 第一轮迭代在用minimum bandwidth, 前一轮产生的脏页数 / 前一轮的周期计算出脏页率(dirty-ing rate), 下一轮的带宽限制 = 脏页率 + 50Mbit/sec 是一个以经验得到的合适公式, 当脏页率大于管理员选择的maximum 或者 少于256KB时, 终止迭代.进入停机拷贝阶段.

		* 在secion6中展示出, 动态调整带宽限制可以在传输大量页面的过程中保持较低的带宽使用, 仅仅在迁移的最后, 传输最热页的WWS时有所增加. 从而有效的对short downtime和average network contention and CPU usage做了平衡.

	- Rapid Page Dirtying

		* 从章节4中我们可以看到, 每个OS workload有一些页的集合被频繁的更新,这个时候每轮都重传这些快速被修改的页面并不合适, 因此我们定期的 偷窥(peek) 一下脏页位图, 仅仅传输本来为脏页, 但在扫描时不为脏页的 页面.

		* 我们进一步观察到脏页常常物理聚集(cluster), 当一个页面变成脏页后, 在不久和它的邻居将变脏, 这个使得我们可能在一个cluster中没有检测到一个页面时, 将检测不到它. 从而依然重传了它, 因此我们采用伪随机方法扫描脏页位图, 来避免这种情况.


	- 半虚拟化优化(Paravirtualized Optimizations)
   
		半虚拟化的一个关键好处是操作系统能够意识到real和virtual environments之间的不同, 在迁移的过程中, 这个允许许多优化, 靠通知操作系统迁移, 
在这一步在OS里的migration stub handler可以通过下列方法帮助提高性能:

		* Stunning Rogue Processes

			在迁移虚拟机的时候, 当内存传输速度快于脏页产生速度时,预拷贝算法是最好的, 但是当有流氓程序存在时, 比如一个test程序, 写一句话到每个page, 可以使得内存脏页率达到320Gbit/sec, 远超网络传输速度. 这种使得预拷贝算法不能持续, 或者迁移不必要延长的程序就是流氓程序.

			无论是managed 还是 self migration, 我们能够阻碍这种危险靠在迁移开始时在OS kernel中forking a monitoring thread, 随着迁移的进行, 这个线程监控监控被要求的每个线程的WWS和行为, 我们实现的简单版本是限制每个进程到40write faults, 实际上我们就占停了那些使迁移困难的进程,  在figure 7中可以看到这个技术工作良好, 但是一个必须注意的问题是不要占停重要的交互式程序.
	
   ![stunning_rogue_processes](./Picture/stunning_rogue_processes.png)


		* Free Page Cache Pages
		一个典型的操作系统常常有大量的free pages, 范围从truly free free(page allocator)到cold buffer cache pages. 当迁移开始时, OS将简单返回所有页到Xen, 这意味这对第一轮迭代传输进行优化, 避免传输free页面.

- Evaluate

	- 这一章我们通过各种workload对我们的实现进行评估, 我们先开始描述我们的测试计划, 然后探索不同负载下的细节. 注意这一章节中没有一个实验使用了上面的半虚拟化优化, 由于我们想测量我们系统的底线性能.

	- Test Setup
		
		* 环境 : 使用两个完全相同的Dell PE-2650 server-class 机器, 每台机器都有双核的Xeon 2GHz CPUS 和 2GB memory. Broadcom TG3的网络接口, 连接到switched Gigabit Ethernet的交换机, 在这些设备中仅仅一个单CPU被使用, HyperThreading被启用. 存储通过iSCSI协议访问存储设备NetApp F840. 在所有的案例中使用XenLinux 2.4.27作为操作系统.

		* Simple Web Server
		
			第一个评估的对象是一个Apache 1.3 web server, 服务静态内容的访问, 在高访问率下被访问. 在100并行客户端访问下, 持续访问512KB文件. Web虚拟机服务器有一个800MB内存的分配.

			在追踪的开始, 服务器实现大约870Mbit/sec的吞吐量, 迁移开始到第27秒钟进行追踪, 但是initially rate-limited到100Mbit/sec(12% CPU), 然后服务器吞吐量下降到765Mbit/s, the initial low-rate 传递传输776MB并且持续62秒, 结合section5的迁移算法, 通过几个迭代增加它的rate, 最终在9.6秒后挂起VM, 最终stop-and-copy阶段传输剩下的pages, 停机165ms使用后恢复full rate.

			这个简单的案例证明了在高负载的服务器上进行迁移, 即控制了对在线服务的影响, 又有一个short的downtime. 然而, 该服务器的工作集是相当小的, 因此该服务利用在线迁移是理想的.

		![simple_web_server](./Picture/simple_web_server.png)

		* Complex Web Workload: SPECweb99
		
			一个更有挑战的Apache workload被SPECweb99呈现, 一个复杂的应用程序级别的基准来评估web服务器和拥有他们的系统, workload是一个复杂的page requests混合: 30%请求内容动态生成, 16%是HTTP POST操作, 0.5%生成一个CGI脚本. 它一般的访问和POST logs都记录到硬盘.

			动态传输速率的调整 + 预拷贝算法 技能保证downtime足够短, 又能保证不争抢服务带宽, 保证服务质量.

		![complex_web_workload](./Picture/complex_web_workload.png)

		* Low-Latency Server: Queke 3
			
			另一个代表性的应用程序的系统是多人在线的游戏服务器, 在这个案例中, 我们配置64M内存的虚拟机环境, 运行一个Quake3服务器. 六个玩家加入游戏共享一个区域, 在这个点下, 我们初始化一个迁移到另一个机器. 这个迁移的细节分析在下图.
		
			第一副图是两次实验中反映数据包接受时间间隔, 反映了中断时间足够的短, 仅50ms左右.

			第二副图反映迭代过程, 与一般情况一样, 每轮的脏页量逐渐缩短,最终脏页率小于阀值时, 跳出迭代.

		![low_latency_quake3_packet](./Picture/low_latency_quake3_packet.png)
	
  
		![low_latency_quake3_pre_copy](./Picture/low_latency_quake3_pre_copy.png)

		* A Diabolical(恶魔的) Workload: MMuncher

			我们评估的最终一个点, 是当脏页率快于网路传输的情况, 我们测试这个恶魔在一个512MB内存中, 一个简单的C程序不断的写道256M内存区域, 这个迁移的结果被呈现在下图. 

			在这个负荷的第一轮迭代中, 我们看见内存的一般都被传输, 然后一半也立即被测试程序污染. 我们的算法尝试调整自己缩放到感知的初始化脏页率, 这个缩放证明是无效的, 随着内存被写的速率变得显然, 在第三轮, 缩放率被扩展到500Mbit/s来最后尝试超过内存写, 随着最后的尝试是不成功, VM被挂起, 剩下的脏页被复制传送, 结果造成了3.5秒的downtime, 幸运的是这样的脏页率在现实的负载中是极其罕见的.

- Future Work

	尽管我们的解决方案十分合适 连接非常合适的数据中心或者集群,并且带着网络访问存储, 有许多区域我们想要放在未来的工作中. 这将允许我们扩展在线迁移到 广域网(wide-area networks), 和环境不能单独依赖于网络附加存储的环境.

	- Cluster Management
	
		一个cluster环境是一个虚拟机的池宿主在一个更小集合的物理服务器上, 有大量的因素动态平衡process, memory 和 network resources. 开发一个cluster控制软件的关键是如何获知VM的放置和移动.
   
		一个特别的案例是从一个节点疏散VM来进行定期的维护. 实现这个明智的方法是按WWS增序迁移虚拟机, 由于每个VM被迁移后释放了在这个节点上占有的资源,额外的CPU和网络对其他VMs变得可能, 可以加快迁移效率. 我们正在建立适合Xen系统的cluster controller.

	- Wide Area Network Redirection

		Our layer 2 redirection scheme works efficiently and with remarkably low outage on modern gigabit networks. 然而, 当迁移出本地子网时, 这个机制不将是有效的, 代替的, 或者OS将不得不获得一个新的IP地址在目的机器上, 或者一些在IP以上的间接层必须存在. 由于这个情形已经与桌面用户相似, 许多不同的解决方案被提出, 较突出中的一个是**Mobile IP**, 在home network(the home agent)的一个节点, 跳转为目的为客户端(mobile node)的数据包到远程网络, 然而这个可能导致性能问题和额外的失败模式.

		还有动态DNS等方法

	- Migrating Block Devices

		尽管NAS在现在的数据中心中很流行, 一些环境一直广泛的使用local disks. 这种情况呈现了一个值得注意的问题, 存储比不稳定内存大得多, 如果硬盘中真个内容必须在迁移前被传输到新的主机中, 整个迁移时间可能被无法容忍的扩展.

		这个延迟可以通过分配硬盘的mirror在一个或者多个远程主机来避免, 例如, 我们研究使用软RAID和Linux的ISCSI功能来在OS迁移前和迁移期间实现硬盘mirror,我们猜想一个软RAID-5的使用, 使得硬盘的数据有更高的可用性. 多个主机可以作为另一个的存储目标, 增加了一些网络传输的代价.

		对于虚拟机的cluster的本地存储的有效管理是一个有趣的问题, 我们希望在未来中有进一步探索. 随着虚拟机来自一个小集合的公共系统image且进行个别的改变在上面, 有机会通过cluster, 管理copy-on-write系统images, 从而促进迁移, 进行复制, 有效的使用了本地存储.


- Conclusion
   
	通过集成live OS migration到Xen VMM中, 我们使得cluster和data centers中interactive workloads能够rapid movement. 我们dynamic network-bandwidth adptation允许迁移过程对运行中的服务产生最小的影响, 并且保证减少总共的downtime到可辨别的阀值以下.

	我们综合的评估展现了例如SPECweb99等现实服务器的worload可以在downtime只有210ms的情况下被迁移, Quake3 game server在毫不被察觉的60msdowntime.

